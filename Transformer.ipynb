{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehaa56789/neural_symbolic_learner_model/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk2vtyhEiGf4"
      },
      "source": [
        "#Purely Neural - TRANSFORMER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnPl9TCVkZcQ"
      },
      "source": [
        "##Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "vM8aakR8lqSd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "1bjsbmD-lvcZ"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# 1. Load & clean\n",
        "# ----------------------\n",
        "df = pd.read_csv(\"train_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "6ZCNkvFHly7O"
      },
      "outputs": [],
      "source": [
        "# Sort per student chronologically\n",
        "df = df.sort_values([\"student_id\", \"start_time\"]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MSwY6n9xC4D",
        "outputId": "59d6b1cf-7760-4e35-e767-d09a4fe4ab03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2319459733.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"correct\"] = df[\"correct\"].fillna(False).astype(bool)\n"
          ]
        }
      ],
      "source": [
        "# Convert correct column to boolean (fill NA with False)\n",
        "df[\"correct\"] = df[\"correct\"].fillna(False).astype(bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "lG-Wvo5J14Dw"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# 2. Handle missing values\n",
        "# ----------------------\n",
        "num_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "cat_cols = df.select_dtypes(include=[\"object\", \"bool\"]).columns\n",
        "\n",
        "# Fill numeric with median\n",
        "for col in num_cols:\n",
        "    if df[col].isnull().any():\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Fill categorical with \"unknown\"\n",
        "for col in cat_cols:\n",
        "    if df[col].isnull().any():\n",
        "        df[col] = df[col].fillna(\"unknown\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "RpC7WVX43M0z"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# 3. Encode categoricals\n",
        "# ----------------------\n",
        "label_encoders = {}\n",
        "for col in [\"problem_type\", \"content_source\", \"skills\", \"tutoring_types\",\"answer_before_tutoring\",\"account_creation_date\"]:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Save encoders\n",
        "with open(\"label_encoders.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoders, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "nVJ5VSwq4U3q"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# 4. Scale numeric features\n",
        "# ----------------------\n",
        "numeric_features = [\n",
        "    \"time_on_task\",\n",
        "    \"fraction_of_hints_used\",\n",
        "    \"attempt_count\",\n",
        "    \"student_answer_count\",\n",
        "    \"mean_correct\",\n",
        "    \"mean_time_on_task\",\n",
        "    \"started_problem_sets_count\",\n",
        "    \"completed_problem_sets_count\",\n",
        "    \"started_skill_builders_count\",\n",
        "    \"mastered_skill_builders_count\",\n",
        "    \"answered_problems_count\",\n",
        "    \"mean_problem_correctness\",\n",
        "    \"mean_problem_time_on_task\",\n",
        "    \"mean_class_score\"\n",
        "]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
        "# Save the fitted scaler\n",
        "with open(\"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "GSPztJXW7CAx",
        "outputId": "4d1db3e2-ed5c-452c-83dc-1f1ec1c35992"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    10000.00000\n",
              "mean        11.04740\n",
              "std          4.59929\n",
              "min          5.00000\n",
              "25%          7.00000\n",
              "50%         10.00000\n",
              "75%         15.00000\n",
              "max         20.00000\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10000.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>11.04740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.59929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>10.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>15.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>20.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "df.groupby(\"student_id\").size().describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUmB2MER74B6",
        "outputId": "cbd51f8b-a6c4-4999-fd64-36c463542547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_id                             int64\n",
            "student_id                         int64\n",
            "assignment_id                      int64\n",
            "problem_id                         int64\n",
            "start_time                        object\n",
            "time_on_task                     float64\n",
            "answer_before_tutoring             int64\n",
            "fraction_of_hints_used           float64\n",
            "attempt_count                    float64\n",
            "answer_given                        bool\n",
            "problem_completed                   bool\n",
            "correct                             bool\n",
            "next_correct                        bool\n",
            "content_source                     int64\n",
            "skills                             int64\n",
            "problem_type                       int64\n",
            "tutoring_types                     int64\n",
            "student_answer_count             float64\n",
            "mean_correct                     float64\n",
            "mean_time_on_task                float64\n",
            "class_id                           int64\n",
            "account_creation_date              int64\n",
            "started_problem_sets_count       float64\n",
            "completed_problem_sets_count     float64\n",
            "started_skill_builders_count     float64\n",
            "mastered_skill_builders_count    float64\n",
            "answered_problems_count          float64\n",
            "mean_problem_correctness         float64\n",
            "mean_problem_time_on_task        float64\n",
            "mean_class_score                 float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "cTe79NHm-ji-"
      },
      "outputs": [],
      "source": [
        "for col in df.select_dtypes(include=\"bool\").columns:\n",
        "    df[col] = df[col].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Oi3Y0kT2ADim"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"transformer_train_data.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "collapsed": true,
        "id": "hoo5VO7S6MLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be8f21d-882c-4601-e25e-a8d96b59427c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 21430\n",
            "One sample X shape: torch.Size([12, 26]) Target: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# 5. Sequence dataset for Transformer\n",
        "# ----------------------\n",
        "class StudentSequenceDataset(Dataset):\n",
        "    def __init__(self, df, context=12, target_col=\"next_correct\"):\n",
        "        self.context = context\n",
        "        self.target_col = target_col\n",
        "\n",
        "        # Group per student\n",
        "        self.groups = []\n",
        "        for sid, group in df.groupby(\"student_id\"):\n",
        "            features = group.drop(columns=[\"start_time\",\"log_id\", \"student_id\", \"account_creation_date\"]).values\n",
        "\n",
        "            targets = group[target_col].astype(int).values\n",
        "\n",
        "            if len(group) < context:\n",
        "                # Pad sequences with zeros\n",
        "                pad_len = context - len(group)\n",
        "                padded_x = np.vstack([np.zeros((pad_len, features.shape[1])), features])\n",
        "                padded_y = targets[-1]  # last available target\n",
        "                self.groups.append((padded_x, padded_y))\n",
        "            else:\n",
        "                for i in range(len(group) - context):\n",
        "                    x = features[i:i+context]\n",
        "                    y = targets[i+context]\n",
        "                    self.groups.append((x, y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.groups)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.groups[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Example usage\n",
        "dataset = StudentSequenceDataset(df, context=12, target_col=\"next_correct\")\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "print(\"One sample X shape:\", dataset[0][0].shape, \"Target:\", dataset[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiTXS_PBAMI_"
      },
      "source": [
        "## Training transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "JIY43jjBBt9P"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "\n",
        "# # ----------------------\n",
        "# # 1. Dataset\n",
        "# # ----------------------\n",
        "# class SAKTDataset(Dataset):\n",
        "#     def __init__(self, df, context=12, target_col=\"next_correct\"):\n",
        "#         self.context = context\n",
        "#         self.samples = []\n",
        "\n",
        "#         # Convert booleans to ints\n",
        "#         for col in df.select_dtypes(include=\"bool\").columns:\n",
        "#             df[col] = df[col].astype(int)\n",
        "\n",
        "#         # Group per student\n",
        "#         for sid, group in df.groupby(\"student_id\"):\n",
        "#             skills = group[\"skills\"].values\n",
        "#             correct = group[\"correct\"].values\n",
        "#             targets = group[target_col].values\n",
        "\n",
        "#             if len(group) < context:\n",
        "#                 pad_len = context - len(group)\n",
        "#                 skills = np.concatenate([np.zeros(pad_len, dtype=int), skills])\n",
        "#                 correct = np.concatenate([np.zeros(pad_len, dtype=int), correct])\n",
        "#                 y = targets[-1]\n",
        "#                 self.samples.append((skills, correct, y))\n",
        "#             else:\n",
        "#                 for i in range(len(group) - context):\n",
        "#                     x_skills = skills[i:i+context]\n",
        "#                     x_correct = correct[i:i+context]\n",
        "#                     y = targets[i+context]\n",
        "#                     self.samples.append((x_skills, x_correct, y))\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.samples)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         skills, correct, y = self.samples[idx]\n",
        "#         return (\n",
        "#             torch.tensor(skills, dtype=torch.long),\n",
        "#             torch.tensor(correct, dtype=torch.long),\n",
        "#             torch.tensor(y, dtype=torch.float32),\n",
        "#         )\n",
        "\n",
        "\n",
        "# # ----------------------\n",
        "# # 2. Model\n",
        "# # ----------------------\n",
        "# class SAKT(nn.Module):\n",
        "#     def __init__(self, num_skills, d_model=32, n_heads=4, num_layers=2):\n",
        "#         super().__init__()\n",
        "#         self.skill_embed = nn.Embedding(num_skills + 1, d_model)\n",
        "#         self.correct_embed = nn.Embedding(2, d_model)\n",
        "\n",
        "#         encoder_layer = nn.TransformerEncoderLayer(\n",
        "#             d_model=d_model, nhead=n_heads, batch_first=True\n",
        "#         )\n",
        "#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "#         self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "#     def forward(self, skills, correct):\n",
        "#         skill_emb = self.skill_embed(skills)\n",
        "#         correct_emb = self.correct_embed(correct)\n",
        "#         x = skill_emb + correct_emb\n",
        "#         x = self.encoder(x)\n",
        "#         x = x[:, -1, :]  # last time step\n",
        "#         out = torch.sigmoid(self.fc(x))\n",
        "#         return out.squeeze(-1)  # keep batch dim\n",
        "\n",
        "\n",
        "# # ----------------------\n",
        "# # 3. Training with Validation\n",
        "# # ----------------------\n",
        "# def train_model(df, num_epochs=10, batch_size=32, lr=1e-3, context=12, val_split=0.2):\n",
        "#     # Dataset\n",
        "#     dataset = SAKTDataset(df, context=context, target_col=\"next_correct\")\n",
        "#     val_size = int(len(dataset) * val_split)\n",
        "#     train_size = len(dataset) - val_size\n",
        "#     train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "#     train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "#     val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "\n",
        "#     # Model\n",
        "#     num_skills = int(df[\"skills\"].max()) + 1\n",
        "#     model = SAKT(num_skills=num_skills, d_model=64, n_heads=4, num_layers=2)\n",
        "\n",
        "#     criterion = nn.BCELoss()\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#     # Train\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         total_loss = 0\n",
        "#         for skills, correct, y in train_loader:\n",
        "#             optimizer.zero_grad()\n",
        "#             preds = model(skills, correct)\n",
        "#             loss = criterion(preds, y)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             total_loss += loss.item() * len(y)\n",
        "\n",
        "#         avg_train_loss = total_loss / train_size\n",
        "\n",
        "#         # Validation\n",
        "#         model.eval()\n",
        "#         all_preds, all_targets = [], []\n",
        "#         with torch.no_grad():\n",
        "#             for skills, correct, y in val_loader:\n",
        "#                 preds = model(skills, correct)\n",
        "#                 all_preds.extend(preds.cpu().numpy())\n",
        "#                 all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "#         val_loss = criterion(torch.tensor(all_preds), torch.tensor(all_targets)).item()\n",
        "#         val_acc = accuracy_score(all_targets, np.round(all_preds))\n",
        "#         try:\n",
        "#             val_auc = roc_auc_score(all_targets, all_preds)\n",
        "#         except ValueError:\n",
        "#             val_auc = float(\"nan\")  # if only 1 class in val targets\n",
        "\n",
        "#         print(\n",
        "#             f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "#             f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "#             f\"- Val Loss: {val_loss:.4f} \"\n",
        "#             f\"- Val Acc: {val_acc:.4f} \"\n",
        "#             f\"- Val AUC: {val_auc:.4f}\"\n",
        "#         )\n",
        "\n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------\n",
        "# 1. Dataset\n",
        "# ----------------------\n",
        "class SAKTDatasetAllFeatures(Dataset):\n",
        "    def __init__(self, df, numeric_cols, categorical_cols, context=12, target_col=\"next_correct\"):\n",
        "        self.context = context\n",
        "        self.samples = []\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.categorical_cols = categorical_cols\n",
        "\n",
        "        # Convert boolean columns to int\n",
        "        for col in df.select_dtypes(include=\"bool\").columns:\n",
        "            df[col] = df[col].astype(int)\n",
        "\n",
        "        # Store categorical mappings (int IDs)\n",
        "        self.cat_maps = {col: sorted(df[col].unique()) for col in categorical_cols}\n",
        "        self.cat_to_idx = {\n",
        "            col: {cat: i for i, cat in enumerate(self.cat_maps[col])} for col in categorical_cols\n",
        "        }\n",
        "\n",
        "        # Group per student\n",
        "        for sid, group in df.groupby(\"student_id\"):\n",
        "            # Convert categorical to IDs\n",
        "            cat_data = []\n",
        "            for col in categorical_cols:\n",
        "                cat_data.append(np.array([self.cat_to_idx[col][v] for v in group[col].values]))\n",
        "            cat_data = np.stack(cat_data, axis=1)  # shape: [seq_len, num_cats]\n",
        "\n",
        "            # Numeric features\n",
        "            num_data = group[numeric_cols].values.astype(np.float32)\n",
        "\n",
        "            # Target\n",
        "            targets = group[target_col].values\n",
        "\n",
        "            # Pad sequences shorter than context\n",
        "            if len(group) < context:\n",
        "                pad_len = context - len(group)\n",
        "                cat_data = np.concatenate([np.zeros((pad_len, len(categorical_cols)), dtype=int), cat_data])\n",
        "                num_data = np.concatenate([np.zeros((pad_len, len(numeric_cols)), dtype=np.float32), num_data])\n",
        "                y = targets[-1]\n",
        "                self.samples.append((num_data, cat_data, y))\n",
        "            else:\n",
        "                for i in range(len(group) - context):\n",
        "                    x_num = num_data[i:i+context]\n",
        "                    x_cat = cat_data[i:i+context]\n",
        "                    y = targets[i+context]\n",
        "                    self.samples.append((x_num, x_cat, y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        num_data, cat_data, y = self.samples[idx]\n",
        "        return (\n",
        "            torch.tensor(num_data, dtype=torch.float32),\n",
        "            torch.tensor(cat_data, dtype=torch.long),\n",
        "            torch.tensor(y, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "# ----------------------\n",
        "# 2. Transformer model\n",
        "# ----------------------\n",
        "class SAKTAllFeatures(nn.Module):\n",
        "    def __init__(self, numeric_dim, categorical_vocab_sizes, d_model=64, n_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Embeddings for categorical features\n",
        "        self.cat_embeddings = nn.ModuleList([\n",
        "            nn.Embedding(vocab_size, d_model) for vocab_size in categorical_vocab_sizes.values()\n",
        "        ])\n",
        "\n",
        "        # Linear projection for numeric features\n",
        "        self.num_linear = nn.Linear(numeric_dim, d_model)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=n_heads, batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, numeric, categorical):\n",
        "        # Embed categorical features\n",
        "        cat_embeds = []\n",
        "        for i, emb_layer in enumerate(self.cat_embeddings):\n",
        "            cat_embeds.append(emb_layer(categorical[:, :, i]))\n",
        "        cat_embeds = torch.stack(cat_embeds, dim=0).sum(dim=0)  # sum over categories\n",
        "\n",
        "        # Project numeric features\n",
        "        num_proj = self.num_linear(numeric)\n",
        "\n",
        "        # Combine numeric + categorical\n",
        "        x = num_proj + cat_embeds\n",
        "\n",
        "        # Transformer encoder\n",
        "        x = self.encoder(x)\n",
        "        x = x[:, -1, :]  # last time step\n",
        "\n",
        "        # Output\n",
        "        out = torch.sigmoid(self.fc(x))\n",
        "        return out.squeeze(-1)\n",
        "\n",
        "# ----------------------\n",
        "# 3. Training with Validation\n",
        "# ----------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "def train_model_all_features(\n",
        "    df,\n",
        "    numeric_cols,\n",
        "    categorical_cols,\n",
        "    num_epochs=10,\n",
        "    batch_size=32,\n",
        "    lr=1e-3,\n",
        "    context=12,\n",
        "    val_split=0.2\n",
        "):\n",
        "    \"\"\"\n",
        "    Train Transformer model (SAKTAllFeatures) using both numeric and categorical features.\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------------\n",
        "    # Dataset\n",
        "    # ----------------------\n",
        "    dataset = SAKTDatasetAllFeatures(df, numeric_cols, categorical_cols, context=context, target_col=\"next_correct\")\n",
        "    val_size = int(len(dataset) * val_split)\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "\n",
        "    # ----------------------\n",
        "    # Model\n",
        "    # ----------------------\n",
        "    categorical_vocab_sizes = {col: df[col].nunique() for col in categorical_cols}\n",
        "    model = SAKTAllFeatures(\n",
        "        numeric_dim=len(numeric_cols),\n",
        "        categorical_vocab_sizes=categorical_vocab_sizes,\n",
        "        d_model=64,\n",
        "        n_heads=4,\n",
        "        num_layers=2\n",
        "    )\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # ----------------------\n",
        "    # Training loop\n",
        "    # ----------------------\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for num_data, cat_data, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(num_data, cat_data)\n",
        "            loss = criterion(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * len(y)\n",
        "\n",
        "        avg_train_loss = total_loss / train_size\n",
        "\n",
        "        # ----------------------\n",
        "        # Validation\n",
        "        # ----------------------\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for num_data, cat_data, y in val_loader:\n",
        "                preds = model(num_data, cat_data)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "        all_preds = np.array(all_preds)\n",
        "        all_targets = np.array(all_targets)\n",
        "\n",
        "        val_loss = criterion(torch.tensor(all_preds), torch.tensor(all_targets)).item()\n",
        "        val_acc = accuracy_score(all_targets, np.round(all_preds))\n",
        "        try:\n",
        "            val_auc = roc_auc_score(all_targets, all_preds)\n",
        "        except ValueError:\n",
        "            val_auc = float(\"nan\")  # e.g. if validation set has only 1 class\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "            f\"- Train Loss: {avg_train_loss:.4f} \"\n",
        "            f\"- Val Loss: {val_loss:.4f} \"\n",
        "            f\"- Val Acc: {val_acc:.4f} \"\n",
        "            f\"- Val AUC: {val_auc:.4f}\"\n",
        "        )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "UfYns2oGtZm2"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "fW4cxsOYByhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7ece66-66e6-439a-9163-0f2220fa62cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 0.5821 - Val Loss: 0.5630 - Val Acc: 0.7177 - Val AUC: 0.8028\n",
            "Epoch 2/10 - Train Loss: 0.5441 - Val Loss: 0.5296 - Val Acc: 0.7296 - Val AUC: 0.8091\n",
            "Epoch 3/10 - Train Loss: 0.5255 - Val Loss: 0.5343 - Val Acc: 0.7343 - Val AUC: 0.8074\n",
            "Epoch 4/10 - Train Loss: 0.5140 - Val Loss: 0.5290 - Val Acc: 0.7298 - Val AUC: 0.8060\n",
            "Epoch 5/10 - Train Loss: 0.5047 - Val Loss: 0.5295 - Val Acc: 0.7359 - Val AUC: 0.8131\n",
            "Epoch 6/10 - Train Loss: 0.4937 - Val Loss: 0.5518 - Val Acc: 0.7345 - Val AUC: 0.8055\n",
            "Epoch 7/10 - Train Loss: 0.4862 - Val Loss: 0.5456 - Val Acc: 0.7214 - Val AUC: 0.8086\n",
            "Epoch 8/10 - Train Loss: 0.4822 - Val Loss: 0.5525 - Val Acc: 0.7305 - Val AUC: 0.8073\n",
            "Epoch 9/10 - Train Loss: 0.4677 - Val Loss: 0.5381 - Val Acc: 0.7352 - Val AUC: 0.8091\n",
            "Epoch 10/10 - Train Loss: 0.4609 - Val Loss: 0.5423 - Val Acc: 0.7308 - Val AUC: 0.8056\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"transformer_train_data.csv\")\n",
        "numeric_cols = [\n",
        "    \"time_on_task\", \"fraction_of_hints_used\", \"attempt_count\",\n",
        "    \"student_answer_count\", \"mean_correct\", \"mean_time_on_task\",\n",
        "    \"started_problem_sets_count\", \"completed_problem_sets_count\",\n",
        "    \"started_skill_builders_count\", \"mastered_skill_builders_count\",\n",
        "    \"answered_problems_count\", \"mean_problem_correctness\",\n",
        "    \"mean_problem_time_on_task\", \"mean_class_score\"\n",
        "]\n",
        "\n",
        "categorical_cols = [\"problem_type\", \"content_source\", \"skills\", \"tutoring_types\"]\n",
        "\n",
        "# Train\n",
        "trained_model = train_model_all_features(df, numeric_cols, categorical_cols, num_epochs=10, batch_size=32, lr=1e-3, context=12)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "NuW1n_3FDVuf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataset, batch_size=64):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for skills, correct, y in loader:\n",
        "            preds = model(skills, correct)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "    # Round predictions for accuracy\n",
        "    acc = accuracy_score(all_targets, np.round(all_preds))\n",
        "\n",
        "    # AUC (may fail if all y are 0 or 1)\n",
        "    try:\n",
        "        auc = roc_auc_score(all_targets, all_preds)\n",
        "    except ValueError:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    return acc, auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQs65jkwDbkz",
        "outputId": "ef172009-7398-4349-fc41-da673e89fe07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy: 0.7831, Final AUC: 0.8665\n"
          ]
        }
      ],
      "source": [
        "# Build dataset again\n",
        "dataset = SAKTDatasetAllFeatures(df, numeric_cols, categorical_cols, context=context, target_col=\"next_correct\")\n",
        "\n",
        "# Evaluate\n",
        "acc, auc = evaluate_model(trained_model, dataset)\n",
        "print(f\"Final Accuracy: {acc:.4f}, Final AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the model"
      ],
      "metadata": {
        "id": "NsAIE4ZxpjDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test dataset preprocessing"
      ],
      "metadata": {
        "id": "YcRHES1Mpnhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"test_data.csv\")\n",
        "df1 = df1.sort_values([\"student_id\", \"start_time\"]).reset_index(drop=True)\n",
        "df1[\"correct\"] = df1[\"correct\"].fillna(False).astype(bool)\n",
        "\n",
        "num_cols = df1.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "cat_cols = df1.select_dtypes(include=[\"object\", \"bool\"]).columns\n",
        "\n",
        "for col in num_cols:\n",
        "    if df1[col].isnull().any():\n",
        "        df1[col] = df1[col].fillna(df1[col].median())\n",
        "\n",
        "for col in cat_cols:\n",
        "    if df1[col].isnull().any():\n",
        "        df1[col] = df1[col].fillna(\"unknown\")\n",
        "\n",
        "\n",
        "# Load encoders and scaler\n",
        "with open(\"label_encoders.pkl\", \"rb\") as f:\n",
        "    label_encoders = pickle.load(f)\n",
        "\n",
        "for col in [\"problem_type\", \"content_source\", \"skills\", \"tutoring_types\",\"answer_before_tutoring\",\"account_creation_date\"]:\n",
        "    df1[col] = df1[col].astype(str)\n",
        "\n",
        "    # Replace unseen categories with \"unknown\"\n",
        "    df1[col] = df1[col].apply(lambda x: x if x in label_encoders[col].classes_ else \"unknown\")\n",
        "\n",
        "    # If \"unknown\" not in classes_, add it\n",
        "    if \"unknown\" not in label_encoders[col].classes_:\n",
        "        label_encoders[col].classes_ = np.append(label_encoders[col].classes_, \"unknown\")\n",
        "\n",
        "    # Transform using training encoder\n",
        "    df1[col] = label_encoders[col].transform(df1[col])\n",
        "\n",
        "\n",
        "\n",
        "with open(\"scaler.pkl\", \"rb\") as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# Transform test data using **fitted scaler**\n",
        "df1[numeric_features] = scaler.transform(df1[numeric_features])\n",
        "\n",
        "df1 = df1.drop(columns=[\"start_time\"])\n",
        "for col in df1.select_dtypes(include=\"bool\").columns:\n",
        "    df1[col] = df1[col].astype(int)\n",
        "\n",
        "df1.to_csv(\"transformer_test_data.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6dctmNQplx7",
        "outputId": "d19755f5-0419-4301-d986-a063a8dba4e9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2856926898.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df1[\"correct\"] = df1[\"correct\"].fillna(False).astype(bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 1. Prepare Test Dataset\n",
        "# ----------------------\n",
        "test_df = pd.read_csv(\"transformer_test_data.csv\")  # preprocessed already\n",
        "context = 12  # same as training\n",
        "\n",
        "numeric_cols = [\n",
        "    \"time_on_task\", \"fraction_of_hints_used\", \"attempt_count\",\n",
        "    \"student_answer_count\", \"mean_correct\", \"mean_time_on_task\",\n",
        "    \"started_problem_sets_count\", \"completed_problem_sets_count\",\n",
        "    \"started_skill_builders_count\", \"mastered_skill_builders_count\",\n",
        "    \"answered_problems_count\", \"mean_problem_correctness\",\n",
        "    \"mean_problem_time_on_task\", \"mean_class_score\"\n",
        "]\n",
        "\n",
        "categorical_cols = [\"problem_type\", \"content_source\", \"skills\", \"tutoring_types\"]\n",
        "\n",
        "\n",
        "test_dataset = SAKTDatasetAllFeatures(\n",
        "    test_df, numeric_cols=numeric_cols, categorical_cols=categorical_cols,\n",
        "    context=context, target_col='next_correct'\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ----------------------\n",
        "# 2. Run Predictions\n",
        "# ----------------------\n",
        "model = trained_model\n",
        "model.eval()\n",
        "\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for numeric, categorical, y in test_loader:\n",
        "        preds = model(numeric, categorical)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "# ----------------------\n",
        "# 3. Convert to 0/1 predictions\n",
        "# ----------------------\n",
        "pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
        "\n",
        "# ----------------------\n",
        "# 4. Evaluate\n",
        "# ----------------------\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "accuracy = accuracy_score(all_targets, pred_labels)\n",
        "auc = roc_auc_score(all_targets, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test AUC: {auc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "n-bGC8yIrXQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0fae200-41eb-4a30-a0d9-65b451ea8d6c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6809\n",
            "Test AUC: 0.7457\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs6NxnB0qqeCY81TOPlcIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}